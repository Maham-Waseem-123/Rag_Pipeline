ğŸ“Œ README â€” RAG Pipeline with ChromaDB, BM25, MMR & Hybrid Retrieval

This repository contains a complete Retrieval-Augmented Generation (RAG) pipeline built using:

LangChain

ChromaDB vector store

SentenceTransformer embeddings

BM25 retriever

MMR retriever

Hybrid retriever

Groq LLM (llama-3.1-8b-instant)

Post-processing tools (summarization, filtering, faithfulness scoring)

The pipeline supports PDF ingestion, chunking, embedding, vector storage, retrieval, hybrid search, and generation.

ğŸš€ 1. Project Overview

This RAG pipeline allows you to:

âœ” Load multiple PDFs
âœ” Split text using different chunking strategies
âœ” Generate sentence embeddings
âœ” Store embeddings in ChromaDB
âœ” Retrieve documents using:

Dense vector search

BM25 (term-based search)

Maximal Marginal Relevance (MMR)

Hybrid BM25 + Embedding retrieval

âœ” Feed retrieved context into Groq LLM
âœ” Generate final answers
âœ” Calculate faithfulness score
âœ” Summarize retrieved passages
âœ” Filter context by keywords

This creates a full end-to-end NLP search + generation system.

ğŸ“¦ 2. Requirements

All required packages are listed in requirements.txt.

langchain>=0.2.0
langchain-community>=0.2.0
langchain-text-splitters
pypdf
pymupdf
sentence-transformers
faiss-cpu
chromadb
langchain-groq
python-dotenv
typesense
langchain-openai
langgraph
PyPDF2
rank_bm25

âœ” Install requirements
pip install --upgrade pip
pip install -r requirements.txt

ğŸ“ 3. Project Structure
/your-project
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/               # input PDFs
â”‚   â””â”€â”€ vector_store/       # persistent ChromaDB storage
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion.py        # PDF ingestion + chunking
â”‚   â”œâ”€â”€ embeddings.py       # embedding manager
â”‚   â”œâ”€â”€ vectorstore.py      # Chroma vector store wrapper
â”‚   â”œâ”€â”€ retrievers.py       # vector, BM25, MMR, hybrid retrieval methods
â”‚   â”œâ”€â”€ rag_pipeline.py     # RAG + Groq model + faithfulness
â”‚   â””â”€â”€ postprocessing.py   # summarization, filtering, query expansion
â”‚
â””â”€â”€ README.md

ğŸ“¥ 4. Data Ingestion
Load all PDFs from a directory

The function:

process_all_pdfs(pdf_directory)


âœ” loads every PDF
âœ” extracts pages
âœ” attaches metadata
âœ” returns LangChain Document objects

ğŸ“‘ 5. Chunking Strategies

You may chunk by:

Page

Paragraph

Sentence

Title-based

Characters

Tokens

Overlapping chunks

Example:

chunk_documents(documents, strategy="sentence")

ğŸ§  6. Embedding Manager

A wrapper over SentenceTransformer:

Loads model (all-MiniLM-L6-v2)

Generates embeddings for text

Returns embedding dimension

Usage:

embedding_manager = EmbeddingManager()
embeddings = embedding_manager.generate_embeddings(text_list)

ğŸ“š 7. ChromaDB Vector Store

The VectorStore class:

Creates persistent folder

Loads/creates ChromaDB collection

Adds documents + embeddings

Example:

vector_store = VectorStore()
vector_store.add_documents(chunks, embeddings)

ğŸ” 8. Retrieval Pipelines
1ï¸âƒ£ Dense Vector Retrieval (Chroma)

Class: RAGRetriever

retriever.retrieve(query, top_k=5)

2ï¸âƒ£ BM25 Retriever (no NLTK)

Class: BM25Retriever

3ï¸âƒ£ Maximal Marginal Relevance (MMR)

Class: MMRRetriever

4ï¸âƒ£ Hybrid Retriever (BM25 + Dense)

Class: HybridRetriever

Each returns:

{
  "id": "...",
  "content": "...",
  "metadata": {...},
  "similarity_score": ...,
  "rank": 1
}

ğŸ¤– 9. Groq LLM Generation

Using llama-3.1-8b-instant through ChatGroq:

llm = ChatGroq(groq_api_key, model_name="llama-3.1-8b-instant")

RAG pipeline:
answer, score = rag_simple_with_faithfulness(query, retriever, llm, embedding_manager)

ğŸ“ 10. Faithfulness Scoring

We compute cosine similarity between:

embedding(context)

embedding(answer)

Score â†’ 0 to 1
Higher = more faithful to the context.

ğŸ›  11. Post Processing
âœ” Summarization (BART)
summarize_passages(retrieved_docs)

âœ” Keyword Filtering
filter_relevant_context(retrieved_docs, ["keyword1", "keyword2"])

âœ” Query Expansion

For retrieval improvement.

ğŸ§ª 12. How to Run End-to-End
# Step 1: Load PDFs
docs = process_all_pdfs("data/pdfs")

# Step 2: Chunk
chunks = chunk_documents(docs, strategy="sentence")

# Step 3: Embeddings
embeddings = embedding_manager.generate_embeddings([c.page_content for c in chunks])

# Step 4: Store in Chroma
vector_store.add_documents(chunks, embeddings)

# Step 5: Initialize retriever
retriever = RAGRetriever(vector_store, embedding_manager)

# Step 6: Ask question
answer, score = rag_simple_with_faithfulness("What is the purpose of the document?", retriever, llm, embedding_manager)

ğŸ“Œ 13. Environment Variables

Create .env file:

GROQ_API_KEY=your_key_here

ğŸ¤ 14. Contributing

Pull Requests are welcome â€” improvements in retrieval, generation, or post-processing are appreciated.

ğŸ“œ 15. License

This project is open-source; feel free to modify and extend.
